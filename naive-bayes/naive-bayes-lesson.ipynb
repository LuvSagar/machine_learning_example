{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Describe Naive Bayes\n",
    "- Choose a Naive Bayes implementation based on your use case\n",
    "- Implement a Naive Bayes model through scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1\">Learning Objectives</a></span></li><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2\">Introduction</a></span></li><li><span><a href=\"#A-Simple-Spam-Example\" data-toc-modified-id=\"A-Simple-Spam-Example-3\">A Simple Spam Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem:-Multiple-Features-Require-Joint-Probabilities\" data-toc-modified-id=\"Problem:-Multiple-Features-Require-Joint-Probabilities-3.1\">Problem: Multiple Features Require Joint Probabilities</a></span></li><li><span><a href=\"#Solution:-The-Naive-Bayes-Independence-Assumption\" data-toc-modified-id=\"Solution:-The-Naive-Bayes-Independence-Assumption-3.2\">Solution: The Naive Bayes Independence Assumption</a></span></li><li><span><a href=\"#Spam-Application:-Multiple-Features\" data-toc-modified-id=\"Spam-Application:-Multiple-Features-3.3\">Spam Application: Multiple Features</a></span></li></ul></li><li><span><a href=\"#Production-Issues\" data-toc-modified-id=\"Production-Issues-4\">Production Issues</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5\">Summary</a></span></li><li><span><a href=\"#Implementation-in-Scikit-Learn\" data-toc-modified-id=\"Implementation-in-Scikit-Learn-6\">Implementation in Scikit-Learn</a></span></li><li><span><a href=\"#Additional-Resources\" data-toc-modified-id=\"Additional-Resources-8\">Additional Resources</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Naive Bayes is a classification algorithm relying on Bayes' rule to assign class labels $y$ for given feature values $X$. As in all other classification models, one is interested in determining the probability of having a certain class label when a certain combination of feature values is obtained. In our case we will be trying to predict the probabilty of our labels given the words that come up on our transaction.\n",
    "\n",
    "\n",
    "$$P(y|X)$$\n",
    "\n",
    "\n",
    "\n",
    "Using Bayes' rule we can write this as\n",
    "\n",
    "$$\n",
    "P(y|X) = \n",
    "\\frac{P(X|y)P(y)}\n",
    "{P(X)}\n",
    "$$\n",
    "\n",
    "which is saying that if we know how likely a certain combination of feature values is within a particular class, if we know how likely that class is to occur, and if we know how likely that combination of feature values is across all classes, we also know how likely the class is given the feature values.\n",
    "\n",
    "That is all we need! If we have enough data with examples for all possible feature values, we just have to look up the respective frequencies in the data and we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Simple Spam Example\n",
    "\n",
    "Suppose we are trying to predict spam emails. For now, we have one feature: whether or not the email mentions \"guarantee.\" All we need to do is to look up what is the fraction of emails containing this word, the fraction of spam emails, and the fraction of spam emails which do contain that word.\n",
    "\n",
    "$G$ = Guarantee, $S$ = Is spam.\n",
    "\n",
    " $$P\\left(S|G\\right) = \\frac{P\\left(G|S\\right)P\\left(S\\right)}{P(G)} = \\frac{P\\left(G|S\\right)P\\left(S\\right)}{P(G|S)P(S) + P(G|\\neg{S})P(\\neg{S})}$$\n",
    "\n",
    "\n",
    "The denominator looks complicated, but it actually isn't. Because $G$ is binary (either present or not), then:\n",
    "\n",
    "> $$P(G) = P(G,S) + P(G,\\neg{S})=P(G|S)P(S) + P(G|\\neg{S})P(\\neg{S})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Multiple Features Require Joint Probabilities\n",
    "\n",
    "In this spam example, we only had one feature: $G$. But, in general, we'll probably use more than one. Really, we want to see some feature vector $X_1, X_2, ..., X_n$ (standing for all the words contained in the email):\n",
    "\n",
    "$$P\\left(S|X_1,\\ldots, X_n\\right) = \\frac{P\\left(X_1,\\ldots, X_n|S\\right)P(S)}{P(X_1,  \\ldots, X_n|S)P(S) + P(X_1, \\ldots, X_n|\\neg{S})P(\\neg S)}$$\n",
    "\n",
    "For example, what is the likelihood that an email is spam given that the email mentions \"guarantee,\" \"oil,\" \"prince,\" and \"million,\" but not \"meeting\", \"colleague,\" and \"dad?\"\n",
    "\n",
    "With a lot of features, calculating the joint probabilities quickly becomes complicated. We would definitely need a lot of data to ensure we have enough feature combinations. \n",
    "If we were interested in the presence or absence of only a 100 selected words, we would have $2^{100}\\approx 10^{30}$ possible feature combinations, and we need to estimate the probability for each of them occurring. The number of possible combinations grows exponentially with the number of features, and we are even just talking about presence or absence.\n",
    "\n",
    "No matter how diligent we are, we may never collect a single training example that contains the precise combination of feature words we need. Therefore, we would be unable to classify a new email containing a particular combination of words.\n",
    "\n",
    "Although each probability is easy to calculate, the joint probability model is simply not practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution: The Naive Bayes Independence Assumption\n",
    "\n",
    "We're stuck, because conditional joint probabilities are required for multiple features. This means there are exponentially more probabilities to compute and data to collect.\n",
    "\n",
    "To get around this, let's make an assumption: \n",
    "\n",
    "> **All $X_i$ are conditionally independent given $S$** \n",
    "\n",
    "(where $S$ indicates \"is spam\"). This means that, given $S$, no two $X_i$ depend on each other. For example, the words \"million\" and \"prince\" each independently indicate that an email is spam. So, it's not the complex interaction of words that determines spam; each feature can indicate whether or not an email is spam independently.\n",
    "\n",
    "\n",
    "Recall that if events $A$ and $B$ are independent, then the joint probability is simply \n",
    "\n",
    "$$P(A,B) = P(A)P(B)$$ \n",
    "\n",
    "Similarly, if $A$ and $B$ are conditionally independent of $S$, then \n",
    "\n",
    "$$P(A,B|S) = P(A|S)P(B|S)$$\n",
    "\n",
    "This formula works out well in general, too:\n",
    "\n",
    "$$P\\left(X_{1}X_{2} \\dots X_{n}|S\\right) = P\\left(X_{1} |S\\right)  P\\left(X_{2} |S\\right) \\cdots P\\left(X_{n} |S\\right)$$\n",
    "\n",
    "> **Caution:** Of course, this assumption is rarely (if ever) true. Often, it requires precise reading to tell whether or not an email was written by a native speaker. In this case, it's often not the particular words used, but how they are used in context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we were considering how indicative the words \"guarantee\" and \"million\" are for being spam, we would write the following\n",
    "\n",
    "$G$ = Guarantee, $M$ = Million, $S$ = Is spam:\n",
    "\n",
    "$$P(S,G,M) = P(G,M|S)P(S) = P(G|S)P(M|S)P(S)$$\n",
    "\n",
    "\n",
    "> None of these probabilities require us to examine multiple features at once in our data set, making them drastically easier to compute. \n",
    "\n",
    "In reality, model parameters/coefficients are unlikely to be independent. But Naive Bayes makes exactly this assumption and often works well despite this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Spam Application: Multiple Features\n",
    "\n",
    "How is this used in practice? Let's combine the Naive Bayes simplification above with our original formula (the denominator is computed the same way as before, combined with our naive assumption):\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "P\\left(S|G,M\\right) &=& \\frac{P(S,G,M)}{P(G,M)} \\\\\n",
    " &=& \\frac{P\\left(G,M|S\\right)P\\left(S\\right)}{P(G,M|S)P(S) + P(G,M|\\neg{S})P(\\neg{S})} \\\\\n",
    " &=& \\frac{P\\left(G|S\\right)P\\left(M|S\\right)P\\left(S\\right)}{P(G|S)P(M|S)P(S) + P(G|\\neg{S})P(M|\\neg{S})P(\\neg{S})}\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    " \n",
    "Typically, we compute this probability for each class (in this case, just $S$ or $\\neg S$), then predict the class with the highest probability. Note that, for all of these, the denominator $P(G,M)$ is constant. Hence, this formula is often written as \"proportional\" ($\\propto$), considerably simplifying it. Instead of comparing the exact probabilities, we can simply see how they score relative to each other. So:\n",
    "\n",
    " $$P\\left(S|GM\\right) \\propto P\\left(G|S\\right)P\\left(M|S\\right)P\\left(S\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Production Issues\n",
    "\n",
    "Accidentally assuming a zero probability for any word appearing in a given class could present major problems — the entire probability estimation would be zero.\n",
    "\n",
    "- **New features:** What if a particular feature was never seen in our training data? Instead of using a zero probability, we should use a technique such as [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to estimate a small non-zero probability.\n",
    "\n",
    "- **Underflow:** Probabilities could be very small if some features rarely occur in some classes. Recall that floating point often gives us trouble because of its limited precision — small floats tend toward zero. We can approach this problem by storing the logarithm of each probability, $P_i$, instead of $P_i$ itself:\n",
    "\n",
    "$$\\log(P_1P_2) = \\log\\ P_1 + \\log\\ P_2$$\n",
    "\n",
    "$$e^{\\log\\ P_1} = P_1$$\n",
    "\n",
    "So: $$P_1P_2 \\cdots P_n = e^{\\log\\ P_1 + \\ldots + \\log\\ P_n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "**Why is the Naive Bayes formula important?** With the independence assumption, we don't need to compute every joint probability distribution, we only need to compute the probability of each word separately: $P(G|S)$ and $P(M|S)$. \n",
    "\n",
    "These calculations can be performed quickly using our training data. The downside is that if spam is actually determined by some complex interaction between \"guarantee\" and \"millions\" (e.g., one of the words on its own is not a good sign of spam, but both in the same email is), then the independence assumption does not hold and our model will not have the capacity to predict spam correctly.\n",
    "\n",
    "To make Naive Bayes a classifier, all we have to do is compute the probability of $P(y|X)$ for each class, $y$. In math notation, this is:\n",
    "\n",
    "$$ P(y \\;|\\; X_1, \\ldots, X_n) \\propto P(y) \\prod_{i=1}^n P(X_i \\;|\\; y) \\\\\n",
    "\\downarrow \\\\\n",
    "\\hat{y} = {\\rm arg} \\; \\underset{y}{\\rm max} \\; P(y) \\prod_{i=1}^n P(X_i \\;|\\; y)$$\n",
    "\n",
    "> The last expression simply means that we predict the class label with the highest predicted probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation in Scikit-Learn\n",
    "\n",
    "- [Docs Bernoulli NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "- [Docs Multinomial NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "- [Docs Gaussian NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "<img src=\"./images/naive-bayes.png\">\n",
    "\n",
    "The differences can be summarized as:\n",
    "-    **BernoulliNB** is designed for binary/Boolean features.\n",
    "-    The **Multinomial Naive Bayes classifier** is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as `tf-idf` may also work.\n",
    "-    **GaussianNB** is designed for continuous, normally distributed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So lets first set up the data for naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"saltanat_labelled_batch_1_18_06_2020.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above read the first batch you labelled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'yapily_transaction_id', 'tracing_id', 'user_id',\n",
       "       'application_id', 'institution_id', 'credit_debit', 'key_path',\n",
       "       'transaction_date', 'transaction_date_str', 'anonymized_at',\n",
       "       'created_at', 'updated_at', 'anonymized_description', 'amount',\n",
       "       'balance', 'balance_type', 'proprietary_bank_transaction_code',\n",
       "       'transaction_id_generation_formula', 'request_date', 'date_accurate',\n",
       "       'labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EATING_OUT', 'NAN', 'OTHER', 'SHOPPING', 'FEES_&_CHARGES',\n",
       "       'TRANSPORT', 'PERSONAL_CARE', 'TRAVEL_&_HOLIDAY', 'ENTERTAINMENT',\n",
       "       'GROCERIES', 'HOME', 'EATING_OUT ', 'PERSONAL CARE',\n",
       "       'GENERAL (PHOTO PRINTING)', 'CHARITY_AND_GIFTS',\n",
       "       'BILLS/FEES_&_CHARGES', 'TRANSOPRT', 'SHOPPING?', 'GENERAL ',\n",
       "       'BILLS', 'SHOPPING/OTHER/FEES_&_CHARGES (the amoun is too small)',\n",
       "       'ENTERTAINMENT/OTHER ', 'INVESTMENT_SAVE', 'INCOME_REFUND',\n",
       "       'INCOME_OTHER', 'INSURANCE_SAVE', 'GENERAL', 'INSURANCE',\n",
       "       'ENTERTAINMENT ', 'EATING-OUT', 'OTHERS'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.labels.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As you can see the above we need to drop the nans out and there are some messy names such as 'BILLS/FEES_&_CHARGES' so lets  drop those as well, the below code will do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.labels=data.labels.apply(lambda x: np.nan if x=='NAN' or x=='nan' or x=='SHOPPING/OTHER/FEES_&_CHARGES (the amoun is too small)' or x=='GENERAL (PHOTO PRINTING)' \n",
    "                              or x=='BILLS/FEES_&_CHARGES'or x=='ENTERTAINMENT/OTHER' or x=='PERSONAL CARE' or x=='INSURANCE_SAVE' or x=='EATING-OUT' or x=='SHOPPING?'or x=='EATING_OUT ' or x=='ENTERTAINMENT/OTHER ' \n",
    "                             or x=='TRANSOPRT'or x=='GENERAL ' or x=='ENTERTAINMENT ' or x=='OTHERS' else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EATING_OUT', 'OTHER', 'SHOPPING', 'FEES_&_CHARGES', 'TRANSPORT',\n",
       "       'PERSONAL_CARE', 'TRAVEL_&_HOLIDAY', 'ENTERTAINMENT', 'GROCERIES',\n",
       "       'HOME', 'CHARITY_AND_GIFTS', 'BILLS', 'INVESTMENT_SAVE',\n",
       "       'INCOME_REFUND', 'INCOME_OTHER', 'GENERAL', 'INSURANCE'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['labels'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4019, 23)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have cleaned up the data lets define our X and y. First lets start with X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(cvec.fit_transform(data.anonymized_description).toarray(),columns=cvec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000000</th>\n",
       "      <th>0006</th>\n",
       "      <th>0006051183</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011462998</th>\n",
       "      <th>0012317699</th>\n",
       "      <th>0024427914</th>\n",
       "      <th>0024427915</th>\n",
       "      <th>0024427916</th>\n",
       "      <th>0032160778</th>\n",
       "      <th>...</th>\n",
       "      <th>yoga</th>\n",
       "      <th>yoomoo</th>\n",
       "      <th>york</th>\n",
       "      <th>youmesushi</th>\n",
       "      <th>your</th>\n",
       "      <th>yoyo</th>\n",
       "      <th>yplan</th>\n",
       "      <th>zac</th>\n",
       "      <th>zebrano</th>\n",
       "      <th>zizzi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1453 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000000  0006  0006051183  0010  0011462998  0012317699  0024427914  \\\n",
       "0       0     0           0     0           0           0           0   \n",
       "1       0     0           0     0           0           0           0   \n",
       "2       0     0           0     0           0           0           0   \n",
       "3       0     0           0     0           0           0           0   \n",
       "4       0     0           0     0           0           0           0   \n",
       "\n",
       "   0024427915  0024427916  0032160778  ...    yoga  yoomoo  york  youmesushi  \\\n",
       "0           0           0           0  ...       0       0     0           0   \n",
       "1           0           0           0  ...       0       0     0           0   \n",
       "2           0           0           0  ...       0       0     0           0   \n",
       "3           0           0           0  ...       0       0     0           0   \n",
       "4           0           0           0  ...       0       0     0           0   \n",
       "\n",
       "   your  yoyo  yplan  zac  zebrano  zizzi  \n",
       "0     0     0      0    0        0      0  \n",
       "1     0     0      0    0        0      0  \n",
       "2     0     0      0    0        0      0  \n",
       "3     0     0      0    0        0      0  \n",
       "4     0     0      0    0        0      0  \n",
       "\n",
       "[5 rows x 1453 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The X above is our transaction Information in numeric format as explained in the CountVectorizer folder.  Each row represents a transaction and the column will represent the words that occured in them. This now means we have our X and if we want to calculate P(Y='Groceries'| X='TESCO') we can do that but first we need to set up the Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Y is our Labels the categories we are trying to predict each transaction in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= naive_bayes.MultinomialNB()\n",
    "model.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above we assingned the naive_bayes.MultinomialNB() as our classifier and the model.fit() will fit this algorithm to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9564568300572281"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model.score() shows us that the accuracy of our model that is using the naives_bayes model we have 95% accuracy, which is pretty good !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EATING_OUT           0.245832\n",
       "SHOPPING             0.143319\n",
       "GROCERIES            0.120926\n",
       "OTHER                0.115700\n",
       "FEES_&_CHARGES       0.085593\n",
       "TRANSPORT            0.080119\n",
       "INVESTMENT_SAVE      0.061956\n",
       "ENTERTAINMENT        0.045036\n",
       "HOME                 0.023389\n",
       "PERSONAL_CARE        0.017417\n",
       "BILLS                0.015924\n",
       "INSURANCE            0.015676\n",
       "GENERAL              0.011943\n",
       "TRAVEL_&_HOLIDAY     0.011197\n",
       "CHARITY_AND_GIFTS    0.005225\n",
       "INCOME_OTHER         0.000498\n",
       "INCOME_REFUND        0.000249\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above shows us the the percentage of our transaction in each class, the baseline model in machine learning will always be the majority class. That is 24% of the data is Eating out so if i predicted all of the transactions in that data as eating out my score or accuracy will be 24%. Our model with naive bayes is 95% so we actually may have a good model. However we should not be fooled as we have not introduced new unseen data or test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As an exercise why don't you try to do the above process but with your labelled batch 2. That is clean your labels up and drop the NANs , set up the X and y and the run the naives_bayes model with your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As an exercise fiddle around with the Countvectorizer and change the ngrams argument. See how setting up your X differently may change your accuracy, this would be like saying P(Y=\"GROCERIES\"|X=\"TESCO STORES\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After that try the model.predict_proba(X) function below and tell me what that means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.78478626e-02, 6.75513602e-03, 5.89276036e-01, 3.59848052e-02,\n",
       "       4.33692725e-02, 1.35198388e-02, 4.73002368e-02, 2.37362036e-02,\n",
       "       6.74453586e-04, 3.41878142e-04, 1.62520537e-02, 3.95367448e-02,\n",
       "       5.15143916e-02, 1.88138726e-02, 4.39459009e-02, 3.76791956e-02,\n",
       "       1.34521174e-02])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [An interesting slide](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf) from a Stanford MOOC that has a section on Naive Bayes\n",
    "- [A much more technical paper](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf) comparing Naive Bayes to logistic regression\n",
    "- [More exposition on Naive Bayes](http://blog.yhat.com/posts/naive-bayes-in-python.html)\n",
    "- [Naive Bayes from scratch](http://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)\n",
    "- [Mixing different variable types in Naive Bayes](https://sebastianraschka.com/faq/docs/naive-bayes-vartypes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
